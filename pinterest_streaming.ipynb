{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d9aff5-13f5-4d38-8ab7-f793a69b1204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define path to the data table\n",
    "creds_path = 'path_to_credentials'\n",
    "# Read the table to a df\n",
    "aws_keys_df = spark.read.format(\"delta\").load(creds_path)\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d469ba-9876-4a97-8d67-8c5df62bc26c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a0f3f6-7b31-4e79-8b5d-0fea5c169086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stream data from Kinesis\n",
    "stream_df = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','Kinesis-Prod-Stream') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1ba73e-77ca-4b31-a23f-836330dcb004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Partitioning streaming data via partition-key\n",
    "# User table\n",
    "user_df = stream_df.filter(stream_df.partitionKey == \"user-partition\")\n",
    "# Pin table\n",
    "pin_df = stream_df.filter(stream_df.partitionKey == \"pin-partition\")\n",
    "# Geo table\n",
    "geo_df = stream_df.filter(stream_df.partitionKey == \"geo-partition\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef89836-3013-4991-be57-75a0acfb3ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Decoding \"data\" column\n",
    "user_df = user_df.selectExpr(\"CAST(data as STRING) UserjsonData\")\n",
    "pin_df = pin_df.selectExpr(\"CAST(data as STRING) PinjsonData\")\n",
    "geo_df = geo_df.selectExpr(\"CAST(data as STRING) GeojsonData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3f87a3-55f9-4158-bfe3-7d506ba1ba5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Structuring the JSON data\n",
    "# User table schema\n",
    "user_struct = StructType([\n",
    "    StructField(\"index\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True)])\n",
    "# Pin table schema\n",
    "pin_struct = StructType([\n",
    "    StructField(\"index\", IntegerType(), False),\n",
    "    StructField(\"unique_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"downloaded\", IntegerType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)])\n",
    "# Geo table schema\n",
    "geo_struct = StructType([\n",
    "    StructField(\"index\", IntegerType(), False),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True),\n",
    "    StructField(\"longitude\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c36b2a-3e0e-4bd7-9d3a-0c9794b3d968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_df = user_df.select(from_json(\"UserjsonData\", user_struct).alias(\"data\")).select(\"data.*\")\n",
    "pin_df = pin_df.select(from_json(\"PinjsonData\", pin_struct).alias(\"data\")).select(\"data.*\")\n",
    "geo_df = geo_df.select(from_json(\"GeojsonData\", geo_struct).alias(\"data\")).select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce350407-e6d2-4427-be4c-9470986c3e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, to_timestamp, concat, lit, regexp_replace, col, when\n",
    "# Clean User streaming data\n",
    "# Creating user_name column by concatenating first_name and last_name\n",
    "user_df = user_df.withColumn(\"user_name\", concat(\"first_name\", lit(\" \"), \"last_name\"))\n",
    "# Dropping first_name and last_name columns\n",
    "user_df = user_df.drop(\"first_name\", \"last_name\")\n",
    "# changing the date_joined column to a timestamp type\n",
    "user_df = user_df.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "# Renaming the index column to ind\n",
    "user_df = user_df.withColumnRenamed(\"index\", \"ind\")\n",
    "# Reordering the columns\n",
    "user_df = user_df.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7943d56-09a4-43af-ac60-41976ccaea14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean Pin streaming data\n",
    "pin_df = pin_df.replace({\"User Info Error\": None})\n",
    "pin_df = pin_df.replace({\"No description available\": None})\n",
    "pin_df = pin_df.replace({\"No description available Story format\": None})\n",
    "pin_df = pin_df.replace({\"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\": None})\n",
    "pin_df = pin_df.replace({\"No Title Data Available\": None})\n",
    "pin_df = pin_df.replace({\"Image src error.\": None})\n",
    "# Amending \"save_location\" to show only the filepath\n",
    "pin_df = pin_df.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "# Replaces the k and M with the appropriate number of zeros and casts the follower_count column to int\n",
    "pin_df = pin_df.withColumn(\n",
    "    \"follower_count\",\n",
    "    when(col(\"follower_count\").endswith(\"k\"), \n",
    "         regexp_replace(col(\"follower_count\"), \"k\", \"\").cast(\"int\") * 1000)\n",
    "    .when(col(\"follower_count\").endswith(\"M\"), \n",
    "         regexp_replace(col(\"follower_count\"), \"M\", \"\").cast(\"int\") * 1000000)\n",
    "    .otherwise(col(\"follower_count\").cast(\"int\"))\n",
    ")\n",
    "# Renamed the index column to ind\n",
    "pin_df = pin_df.withColumnRenamed(\"index\", \"ind\")\n",
    "# Casted these two columns to int since they were originally long numerical type which wasn't neccesary for these smaller numbers (although in a real life situation i imagine it would be better to keep them as long)\n",
    "\n",
    "pin_df = pin_df.withColumn(\"ind\", pin_df[\"ind\"].cast(\"int\"))\n",
    "# Reordering Columns (seems downloaded column is being dropped aswell?!!?!?!?!?!?!?!)\n",
    "pin_df = pin_df.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a9d9b69-9f95-41aa-8082-2b6e7b7dc4bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean Geo streaming data\n",
    "# making a coordinates column from longitude and latitude columns using an array\n",
    "geo_df = geo_df.withColumn(\"coordinates\", array(\"latitude\", \"longitude\"))\n",
    "geo_df = geo_df.drop(\"latitude\", \"longitude\")\n",
    "# changing the timestamp column to a timestamp type\n",
    "geo_df = geo_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "# Renaming the index column to ind\n",
    "geo_df = geo_df.withColumnRenamed(\"index\", \"ind\")\n",
    "# Reordering the columns\n",
    "geo_df = geo_df.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6cc9d09-8f95-4af7-a9db-5918eb1883ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the stream data to Delta tables\n",
    "user_df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"c1b2415b9314_user_table\")\n",
    "\n",
    "pin_df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"c1b2415b9314_pin_table\")\n",
    "\n",
    "geo_df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"c1b2415b9314_geo_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4783f5d0-817b-457b-832c-9d4809d24340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete checkpoint folder in order to use writeStream again\n",
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8679340985952443,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Kinesis Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
